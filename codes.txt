main.py:

import tabs
import test
import sys
def main():
    tabs.tabs()
    test.test()
    
if __name__ == "__main__":
    main()


tabs.py:

import cv2
from cvzone.HandTrackingModule import HandDetector
import pygetwindow as gw
import tensorflow as tf
import sys

from keras.models import load_model
# Load the model
new_model = tf.keras.models.load_model("C:\\Users\DELL 7480\gesture_model_1.h5")
# Parameters
width = 720
gestureThreshold = 150



# Camera Setup
cap = cv2.VideoCapture(0)
cap.set(3, width)

# Hand Detector
detectorHand = HandDetector(detectionCon=0.8, maxHands=1)

# List to keep track of minimized windows
minimized_windows = []

# Open the output file for writing
output_file = open('output.txt', 'w')

while True:
    # Get image frame
    success, img = cap.read()
    img = cv2.flip(img, 1)

    # Find the hand and its landmarks
    hands, img = detectorHand.findHands(img)  

    if hands:  # If hand is detected
        hand = hands[0]
        cx, cy = hand["center"]
        lmList = hand["lmList"]  # List of 21 Landmark points
        fingers = detectorHand.fingersUp(hand)  # List of which fingers are up

        if cy <= gestureThreshold:  # If hand is at the height of the face
            # If index finger and thumb are up (pinch gesture), minimize the active window
            if fingers == [1, 0, 0, 0, 1]:
                active_window = gw.getActiveWindow()
                if active_window is not None and active_window not in minimized_windows:
                    active_window.minimize()
                    minimized_windows.append(active_window)
                    output_file.write("0\n")  # Write 0 to indicate minimize


            # If all fingers are up (open palm), close the active window
            elif fingers == [0, 1, 0, 0, 1]:
                active_window = gw.getActiveWindow()
                if active_window is not None:
                    active_window.close()
                    output_file.write("1\n")  # Write 1 to indicate close

            # If index and middle fingers are up (V sign), restore down the last minimized window
            elif fingers == [1, 1, 0, 0, 0]:
                if minimized_windows:
                    last_minimized = minimized_windows.pop()
                    last_minimized.restore()
                    output_file.write("2\n")  # Write 2 to indicate restore

            # If thumb and index finger are up (OK sign), maximize the active window
            elif fingers == [1, 1, 0, 0, 1]:
                active_window = gw.getActiveWindow()
                if active_window is not None:
                    active_window.maximize()
                    output_file.write("3\n")  # Write 3 to indicate maximize

    cv2.imshow("Camera", img)

    key = cv2.waitKey(1)
    if key == ord('q'):
        break

# Release the camera and close OpenCV windows
cap.release()
cv2.destroyAllWindows()

# Close the output file
output_file.close()

test.py:

import cv2
import mediapipe as mp
import pyautogui
import time
import tensorflow as tf
import sys

from keras.models import load_model
# Load the model
new_model = tf.keras.models.load_model("C:\\Users\DELL 7480\gesture_model_1.h5")

mute_state = False  # Initialize mute state as False

# Function to record actions to a file
def record_action(action):
    with open('output.txt', 'a') as output_file:
        output_file.write(action + "\n")
def count_fingers(lst):
    cnt = 0

    thresh = (lst.landmark[0].y * 100 - lst.landmark[9].y * 100) / 2

    if (lst.landmark[5].y * 100 - lst.landmark[8].y * 100) > thresh:
        cnt += 1

    if (lst.landmark[9].y * 100 - lst.landmark[12].y * 100) > thresh:
        cnt += 1

    if (lst.landmark[13].y * 100 - lst.landmark[16].y * 100) > thresh:
        cnt += 1

    if (lst.landmark[17].y * 100 - lst.landmark[20].y * 100) > thresh:
        cnt += 1

    if (lst.landmark[5].x * 100 - lst.landmark[4].x * 100) > 6:
        cnt += 1

    return cnt


cap = cv2.VideoCapture(0)

drawing = mp.solutions.drawing_utils
hands = mp.solutions.hands.Hands(max_num_hands=1)
hand_obj = hands

start_init = False
prev = -1


try:
    while True:
        _, frm = cap.read()
        frm = cv2.flip(frm, 1)

        res = hand_obj.process(cv2.cvtColor(frm, cv2.COLOR_BGR2RGB))

        if res.multi_hand_landmarks:
            hand_keyPoints = res.multi_hand_landmarks[0]
            cnt = count_fingers(hand_keyPoints)

            if not (prev == cnt):
                if not start_init:
                    start_time = time.time()
                    start_init = True
                elif (time.time() - start_time) > 0.1:
                    if cnt == 1:
                        pyautogui.press("right")
                        record_action("forward")
                    elif cnt == 2:
                        pyautogui.press("left")
                        record_action("backward")
                    elif cnt == 3:
                        pyautogui.press("up")
                        record_action("volume up")
                    elif cnt == 4:
                        pyautogui.press("down")
                        record_action("volume down")
                    elif cnt == 5:
                        pyautogui.press("space")
                        record_action("play/pause")

                    prev = cnt
                    start_init = False

            drawing.draw_landmarks(frm, hand_keyPoints, mp.solutions.hands.HAND_CONNECTIONS)

        cv2.imshow("Camera", frm)

        if cv2.waitKey(1) == 27:
            break
        key = cv2.waitKey(1)
        if key == ord('q'):
            break
except KeyboardInterrupt:
    cap.release()
    cv2.destroyAllWindows()
