# GAP-Gesture-Application-Protocol-
I have worked on a Machine Learning Model named as GAP(Gesture Application Protocol) or Wireless Application Control using Hand Gestures and processed by ML algorithms.This model can give accurate prediction of gesture actions
— Hand gestures provide a natural way for gadgets in a more practical and relaxing manner while also humans to interact with computers to perform a variety of increasing productivity. different applications. The human-computer interaction became an importance in bringing the idea that the link
between a user and a computer should look more and more B. Computer vision, Deep learning, Machine learning like one between two human beings. Thus, with increasing uses in Gestures improvement in technology, response time and ease of operations are the concerns. Here is where human-computer interaction comes into show. This interaction is open and • The identification, interpretation, and use of gestures challenges the used devices such as the keyboard and mouse across a variety of applications depend heavily on the for input. Gestures are natural and are frequently used in day- underlying technologies of computer vision, deep to-day communications. Therefore, communicating using learning, and machine learning. gestures with computers creates a whole new standard of interaction. In this project, with the help of computer vision 1) Computer vision: In order to replicate human visual and deep learning techniques, user hand movements (gestures) perception, the discipline of computer vision involves are used to control the media player, PPT slides, minimizing the processing and interpretation of visual input from and closing the applications etc. The proposed web application
enables the user to use their local device camera to identify the outside environment. Computer vision techniques their gesture and execute the control over the media player and are used to record and decipher the motions and similar applications (without any additional hardware). It patterns of human gestures in the context of gestures. increases effectiveness and makes communication easy in
allowing the user control his/her laptop/desktop from a i. Hand Detection and Tracking: Computer vision particular distance. We are thinking that in future people will algorithms are employed to identify and track move to wireless technologies and gestures are part of that so, hands in video feeds or images, determining their we have made our choice to do this project. positions and movements.
4.	GoogleNet.
5.	MobileNet.
6.	VGG.
• Working of CNN model in this project
Importing necessary libraries, such as Tensor Flow, Keras, NumPy, Pandas, Matplotlib, Seaborn, and OS, is required for code commencement. It specifies the locations of the training and testing data folders and uses Matplotlib's imshow function to display a selection of randomly chosen pictures from the training dataset that represent different classes. For model training, variables such as batch size, picture dimensions, the number of classes, and epochs are specified. By adding transformations like rotation, shifting, shearing, zooming, and flipping to the training dataset, the Keras Image Data Generator is used to improve the generalization skills of the model. During model training, this generator makes it easier for data to flow from the training and testing directories. Utilizing the Sequential API of Keras, the CNN model is built. Convolutional and MaxPooling layers are interspersed with flattened and fully linked (Dense) levels before the final layer. Dropout regularization is used to stop overfitting from happening. The categorical cross-entropy loss function and Adam optimizer are used to create the model. The fit function is used to train the model, recording the training history along with accuracy and loss values for both the training and validation sets. After training, the model is assessed using testing data, and its accuracy is shown. The accuracy curves for training and validation are displayed using Matplotlib. To calculate accuracy metrics, the model's predictions on the test data are obtained and contrasted with the actual class labels. Using the Scikit-Learn classification_report function, a classification report is produced that includes the precision, recall, F1-score, and support for each class. Using Seaborn's heatmap, a confusion matrix is created to show how the model's predictions compare to the actual labels. The code describes the whole procedure for developing, training, and assessing a CNN model for image classification, including data preparation, model design, training loop, performance assessment, and performance visualization. This provides insights into the model's accuracy and its effectiveness in classifying images within various categories.
	2.	VGG 16 CNN Model
The Visual Geometry Group at the University of Oxford created the Convolutional Neural Network (CNN) architecture known as VGG-16, which stands for Visual Geometry Group 16. It is praised for its simple yet efficient method for handling picture categorization challenges.
VGG-16 CNN model architecture overview:
There are 16 layers in the VGG-16, including 13 convolutional layers and 3 fully linked layers. The convolutional layers are divided into five groups, with maxpooling layers after each convolutional layer in each group. The picture categorization is carried out by the last trio of layers that are fully integrated.	1.
.
•	Convolutional and Pooling Layers: In VGG-16, the convolutional layers employ small filters, typically 3x3 in size, to extract diverse features from the input image. As the network deepens, it becomes capable of capturing more abstract and intricate features. Subsequent to each convolutional layer, a max-pooling layer is introduced to reduce the spatial dimensions of the feature maps, thereby reducing computational demands and broadening the receptive field.
Filter Sizes and Depth: Throughout VGG-16, 3x3 filters are consistently used. Initially, the model maintains a uniform depth of 64 filters for the first two convolutional groups, which then doubles in depth for the subsequent groups (128, 256, 512, and 512). This depth progression equips the model to recognize increasingly complex patterns.
Fully Connected Layers: Following the convolutional layers, VGG-16 appends three fully connected layers, each comprising 4096 units. These layers amalgamate the high-level features learned by the convolutional layers and perform the ultimate classification.
Activation Function: Rectified Linear Unit (ReLU) activation functions are applied after each convolutional and fully connected layer. ReLU introduces nonlinearity into the network, enabling it to learn intricate mappings between inputs and outputs.
Dropout: To mitigate overfitting, VGG-16 employs dropout regularization within the fully connected layers. Dropout randomly deactivates a fraction of neurons during training, thereby fostering network robustness and reducing reliance on specific neurons.
Softmax Activation: The final layer of the network utilizes the softmax activation function to yield class probabilities, rendering VGG-16 suitable for multi-class classification tasks.
Output Layer: The output layer possesses a number of units equivalent to the classes within the classification problem. The predicted class corresponds to the one with the highest probability within the softmax output.
Image Preprocessing: Typically, input images are resized to a fixed dimension, such as 224x224, and are normalized by subtracting the mean pixel values.
VGG-16's principal contribution lies in demonstrating the efficacy of deep convolutional networks in image recognition tasks. Nevertheless, its architectural depth and parameter count make it computationally intensive and prone to overfitting when applied to smaller datasets. Despite these limitations, VGG-16 has laid the foundation for even deeper and more efficient architectures, such as ResNet and Inception, which build upon VGG's principles while introducing additional innovations.
It is clear by comparing the CNN and VGG-16 models using their respective classification reports that the VGG-16 model performs better than the CNN model in terms of accuracy and F1-scores. The accuracy of the VGG-16 model was 91.00%, above that of the CNN model, which was
81.85%. Additionally, it demonstrated a steady and reliable performance with F1-scores ranging from 0.88 to 1.00, producing a solid macro average of 0.92. An outstanding example of the VGG-16 model's accuracy in categorizing the "none" category is its flawless precision and recall. The CNN model, on the other hand, performed admirably with an overall F1-score of 0.92, although it showed higher variation in accuracy and recall between classes. The choice between both models should take into account the particular job requirements. The VGG-16 model may be selected for improved overall accuracy and dependability in class predictions, while additional fine-tuning of the CNN model might be investigated to solve specific class problems, if necessary.

This study presents a revolutionary hand gesture recognition-based system for controlling media players, PowerPoint slides, and window management (minimizing, maximizing, shutting, and restoring). 

The approach uses Open-CV methods to capture pictures, a 2-Dimensional Convolutional Neural Network (2D CNN) for feature extraction and gesture prediction, and PyAuto-GUI to provide keyboard control upon gesture recognition. Realtime testing was done on the suggested model using a custom dataset of 10 different gestures, and the results showed an amazing accuracy rate of 91.00% when utilizing the VGG-16 CNN architecture. This technology offers a user-friendly and economical way to interface with computer systems. Enhancing gesture recognition skills in many contexts, such as those related to artificial intelligence (AI) and the medical industry, may be the focus of future initiatives.
